#!/usr/bin/env python3
"""
å°çº¢ä¹¦ç¬”è®°æŠ“å– - åŸºäº XHS-Downloader çš„è§£æé€»è¾‘
ç›´æ¥ä½¿ç”¨ requests è¯·æ±‚ï¼Œé¿å… httpx çš„ä»£ç†é—®é¢˜

ä½¿ç”¨æ–¹æ³•:
    python xhs_fetch.py "ç¬”è®°URLæˆ–ID" --output /tmp/xhs_output --ocr
"""

import argparse
import json
import os
import re
import requests
import subprocess
from pathlib import Path
from datetime import datetime
from lxml.etree import HTML
from yaml import safe_load


def load_cookies():
    """åŠ è½½ä¿å­˜çš„ cookies"""
    cookie_file = Path.home() / '.xiaohongshu-scraper' / 'cookie_string.txt'
    if cookie_file.exists():
        return cookie_file.read_text().strip()
    
    # å°è¯•ä» JSON æ ¼å¼åŠ è½½
    json_file = Path.home() / '.xiaohongshu-scraper' / 'cookies.json'
    if json_file.exists():
        cookies = json.loads(json_file.read_text())
        return '; '.join([f"{c['name']}={c['value']}" for c in cookies])
    
    return None


def extract_note_id(url_or_id):
    """ä»å„ç§æ ¼å¼çš„é“¾æ¥ä¸­æå–ç¬”è®° ID"""
    if re.match(r'^[a-f0-9]{24}$', url_or_id):
        return url_or_id
    
    patterns = [
        r'/explore/([a-f0-9]{24})',
        r'/discovery/item/([a-f0-9]{24})',
        r'/item/([a-f0-9]{24})',
        r'user/profile/[a-z0-9]+/([a-f0-9]{24})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url_or_id)
        if match:
            return match.group(1)
    return None


def resolve_short_link(short_url, headers):
    """è§£æçŸ­é“¾æ¥"""
    try:
        resp = requests.get(short_url, headers=headers, allow_redirects=False, timeout=10)
        if resp.status_code in [301, 302]:
            return resp.headers.get('Location', '')
    except:
        pass
    return short_url


def fetch_html(url, cookie):
    """è·å–é¡µé¢ HTML"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cookie': cookie,
    }
    
    # å¤„ç†çŸ­é“¾æ¥
    if 'xhslink.com' in url:
        url = resolve_short_link(url, headers)
    
    resp = requests.get(url, headers=headers, timeout=30)
    return resp.text


def parse_initial_state(html):
    """ä» HTML ä¸­æå– __INITIAL_STATE__ æ•°æ®"""
    html_tree = HTML(html)
    scripts = html_tree.xpath("//script/text()")
    
    for script in reversed(scripts):
        if script.startswith("window.__INITIAL_STATE__"):
            data = safe_load(script.lstrip("window.__INITIAL_STATE__="))
            
            # PC ç«¯æ•°æ®ç»“æ„
            if 'note' in data and 'noteDetailMap' in data['note']:
                note_map = data['note']['noteDetailMap']
                if note_map:
                    # éå†æ‰€æœ‰ keyï¼ˆå¯èƒ½æ˜¯ undefined æˆ–å®é™… note_idï¼‰
                    for key, value in note_map.items():
                        if value and isinstance(value, dict) and 'note' in value:
                            note_data = value.get('note', {})
                            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
                            if note_data.get('noteId') or note_data.get('title') or note_data.get('desc'):
                                return note_data
            
            # ç§»åŠ¨ç«¯æ•°æ®ç»“æ„
            if 'noteData' in data:
                return data['noteData'].get('data', {}).get('noteData', {})
    
    return None


def extract_note_data(data):
    """ä»è§£æçš„æ•°æ®ä¸­æå–ç¬”è®°ä¿¡æ¯"""
    result = {
        'note_id': data.get('noteId', ''),
        'title': data.get('title', ''),
        'desc': data.get('desc', ''),
        'type': data.get('type', ''),  # video æˆ– normal
        'author': {
            'nickname': data.get('user', {}).get('nickname', ''),
            'user_id': data.get('user', {}).get('userId', ''),
        },
        'interact': {
            'liked_count': data.get('interactInfo', {}).get('likedCount', 0),
            'collected_count': data.get('interactInfo', {}).get('collectedCount', 0),
            'comment_count': data.get('interactInfo', {}).get('commentCount', 0),
            'share_count': data.get('interactInfo', {}).get('shareCount', 0),
        },
        'tags': [tag.get('name', '') for tag in data.get('tagList', [])],
        'time': data.get('time', 0),
        'images': [],
        'video': None,
    }
    
    # æå–å›¾ç‰‡
    image_list = data.get('imageList', [])
    for img in image_list:
        # ä¼˜å…ˆä½¿ç”¨åŸå›¾
        url = img.get('urlDefault') or img.get('url', '')
        if url:
            # ç§»é™¤æ°´å°å‚æ•°ï¼Œè·å–é«˜æ¸…å›¾
            url = re.sub(r'\?.*$', '', url)
            result['images'].append({
                'url': url,
                'width': img.get('width', 0),
                'height': img.get('height', 0),
            })
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŠ¨å›¾
        if img.get('livePhoto'):
            result['images'][-1]['live_photo'] = img.get('stream', {}).get('h264', [{}])[0].get('masterUrl', '')
    
    # æå–è§†é¢‘
    if data.get('type') == 'video':
        video_info = data.get('video', {})
        media = video_info.get('media', {})
        stream = media.get('stream', {})
        
        # è·å–æœ€é«˜æ¸…çš„è§†é¢‘
        for quality in ['h266', 'h265', 'h264', 'av1']:
            if quality in stream and stream[quality]:
                video_list = stream[quality]
                if video_list:
                    result['video'] = {
                        'url': video_list[0].get('masterUrl', ''),
                        'quality': quality,
                    }
                    break
    
    return result


def download_file(url, save_path):
    """ä¸‹è½½æ–‡ä»¶"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Referer': 'https://www.xiaohongshu.com/'
    }
    
    try:
        resp = requests.get(url, headers=headers, timeout=60, stream=True)
        if resp.status_code == 200:
            with open(save_path, 'wb') as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)
            return True
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥: {e}")
    return False


def ocr_image_vision(image_path):
    """ä½¿ç”¨ macOS Vision æ¡†æ¶è¿›è¡Œ OCR"""
    swift_code = '''
import Vision
import AppKit

let imagePath = CommandLine.arguments[1]
guard let image = NSImage(contentsOfFile: imagePath),
      let cgImage = image.cgImage(forProposedRect: nil, context: nil, hints: nil) else {
    exit(0)
}

let request = VNRecognizeTextRequest { request, error in
    guard let observations = request.results as? [VNRecognizedTextObservation] else { return }
    for observation in observations {
        if let topCandidate = observation.topCandidates(1).first {
            print(topCandidate.string)
        }
    }
}
request.recognitionLevel = .accurate
request.recognitionLanguages = ["zh-Hans", "zh-Hant", "en"]
let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
try? handler.perform([request])
'''
    swift_file = '/tmp/ocr_vision.swift'
    with open(swift_file, 'w') as f:
        f.write(swift_code)
    
    try:
        result = subprocess.run(['swift', swift_file, image_path], capture_output=True, text=True, timeout=60)
        return result.stdout.strip() if result.returncode == 0 else None
    except:
        return None


def main():
    parser = argparse.ArgumentParser(description='å°çº¢ä¹¦ç¬”è®°æŠ“å–')
    parser.add_argument('url', help='ç¬”è®° URL æˆ– ID')
    parser.add_argument('--output', '-o', default='/tmp/xhs_note', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--ocr', action='store_true', help='å¯¹å›¾ç‰‡è¿›è¡Œ OCR')
    parser.add_argument('--cookie', '-c', help='Cookie å­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»æ–‡ä»¶è¯»å–ï¼‰')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("å°çº¢ä¹¦ç¬”è®°æŠ“å–")
    print("=" * 60)
    
    # åŠ è½½ cookie
    cookie = args.cookie or load_cookies()
    if not cookie:
        print("âŒ æœªæ‰¾åˆ° Cookieï¼Œè¯·å…ˆç™»å½•å°çº¢ä¹¦")
        return
    
    print("âœ“ Cookie å·²åŠ è½½")
    
    # æå–ç¬”è®° ID
    note_id = extract_note_id(args.url)
    if not note_id:
        # å¯èƒ½æ˜¯çŸ­é“¾æ¥ï¼Œå…ˆè§£æ
        print(f"å°è¯•è§£æé“¾æ¥: {args.url}")
    
    # æ„å»º URL
    if note_id:
        url = f"https://www.xiaohongshu.com/explore/{note_id}"
    else:
        url = args.url
    
    print(f"ç›®æ ‡ URL: {url}")
    
    # è·å–é¡µé¢
    print("\næ­£åœ¨è·å–é¡µé¢...")
    try:
        html = fetch_html(url, cookie)
    except Exception as e:
        print(f"âŒ è·å–é¡µé¢å¤±è´¥: {e}")
        return
    
    # è§£ææ•°æ®
    print("æ­£åœ¨è§£ææ•°æ®...")
    data = parse_initial_state(html)
    
    if not data:
        print("âŒ è§£ææ•°æ®å¤±è´¥")
        # ä¿å­˜ HTML ç”¨äºè°ƒè¯•
        debug_file = Path(args.output) / 'debug.html'
        debug_file.parent.mkdir(parents=True, exist_ok=True)
        debug_file.write_text(html)
        print(f"å·²ä¿å­˜ HTML åˆ°: {debug_file}")
        return
    
    # æå–ç¬”è®°ä¿¡æ¯
    note = extract_note_data(data)
    
    print(f"\nğŸ“ æ ‡é¢˜: {note['title']}")
    print(f"ğŸ‘¤ ä½œè€…: {note['author']['nickname']}")
    print(f"ğŸ“„ æè¿°: {note['desc'][:100]}..." if len(note['desc']) > 100 else f"ğŸ“„ æè¿°: {note['desc']}")
    print(f"â¤ï¸  ç‚¹èµ: {note['interact']['liked_count']} | æ”¶è—: {note['interact']['collected_count']} | è¯„è®º: {note['interact']['comment_count']}")
    print(f"ğŸ·ï¸  æ ‡ç­¾: {', '.join(note['tags'])}")
    print(f"ğŸ–¼ï¸  å›¾ç‰‡: {len(note['images'])} å¼ ")
    if note['video']:
        print(f"ğŸ¬ è§†é¢‘: {note['video']['quality']}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    images_dir = output_dir / 'images'
    images_dir.mkdir(exist_ok=True)
    
    # ä¸‹è½½å›¾ç‰‡
    local_images = []
    if note['images']:
        print(f"\næ­£åœ¨ä¸‹è½½ {len(note['images'])} å¼ å›¾ç‰‡...")
        for i, img in enumerate(note['images']):
            url = img['url']
            ext = '.webp' if '.webp' in url else '.jpg'
            filename = f"image_{i+1:02d}{ext}"
            save_path = images_dir / filename
            
            if download_file(url, str(save_path)):
                local_images.append(str(save_path))
                print(f"  âœ“ {filename}")
            else:
                print(f"  âœ— {filename}")
    
    # ä¸‹è½½è§†é¢‘
    local_video = None
    if note['video']:
        print("\næ­£åœ¨ä¸‹è½½è§†é¢‘...")
        video_path = output_dir / 'video.mp4'
        if download_file(note['video']['url'], str(video_path)):
            local_video = str(video_path)
            print(f"  âœ“ video.mp4")
    
    # OCR
    ocr_results = []
    if args.ocr and local_images:
        print("\næ­£åœ¨è¿›è¡Œ OCR è¯†åˆ«...")
        for img_path in local_images:
            img_name = Path(img_path).name
            print(f"  å¤„ç†: {img_name}")
            text = ocr_image_vision(img_path)
            if text:
                ocr_results.append({
                    'image': img_name,
                    'text': text
                })
                print(f"    è¯†åˆ«åˆ° {len(text)} å­—ç¬¦")
    
    # ä¿å­˜ç»“æœ
    result = {
        'note_id': note['note_id'],
        'fetch_time': datetime.now().isoformat(),
        'title': note['title'],
        'desc': note['desc'],
        'author': note['author'],
        'interact': note['interact'],
        'tags': note['tags'],
        'images': note['images'],
        'video': note['video'],
        'local_images': local_images,
        'local_video': local_video,
        'ocr_results': ocr_results,
    }
    
    with open(output_dir / 'note.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆ Markdown
    md = f"""# {note['title']}

**ä½œè€…**: [{note['author']['nickname']}](https://www.xiaohongshu.com/user/profile/{note['author']['user_id']})  
**ç‚¹èµ**: {note['interact']['liked_count']} | **æ”¶è—**: {note['interact']['collected_count']} | **è¯„è®º**: {note['interact']['comment_count']}  
**æ ‡ç­¾**: {', '.join(['#' + t for t in note['tags']])}  
**æŠ“å–æ—¶é—´**: {result['fetch_time']}

---

## æ­£æ–‡

{note['desc']}

---

## å›¾ç‰‡

"""
    
    for i, img_path in enumerate(local_images):
        img_name = Path(img_path).name
        md += f"### å›¾ç‰‡ {i+1}\n\n"
        md += f"![{img_name}](images/{img_name})\n\n"
        
        for ocr in ocr_results:
            if ocr['image'] == img_name:
                md += f"**OCR è¯†åˆ«æ–‡å­—:**\n\n```\n{ocr['text']}\n```\n\n"
                break
    
    if local_video:
        md += f"\n## è§†é¢‘\n\n[video.mp4](video.mp4)\n"
    
    with open(output_dir / 'note.md', 'w', encoding='utf-8') as f:
        f.write(md)
    
    print(f"\n" + "=" * 60)
    print("âœ… æŠ“å–å®Œæˆ!")
    print("=" * 60)
    print(f"\nè¾“å‡ºç›®å½•: {output_dir}")
    print(f"  - note.json    (ç»“æ„åŒ–æ•°æ®)")
    print(f"  - note.md      (Markdown æ–‡æ¡£)")
    print(f"  - images/      ({len(local_images)} å¼ å›¾ç‰‡)")
    if local_video:
        print(f"  - video.mp4    (è§†é¢‘)")
    if ocr_results:
        print(f"  - OCR ç»“æœå·²åŒ…å«åœ¨ JSON å’Œ MD ä¸­")


if __name__ == '__main__':
    main()
