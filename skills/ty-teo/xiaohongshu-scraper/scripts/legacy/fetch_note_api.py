#!/usr/bin/env python3
"""
å°çº¢ä¹¦ç¬”è®°æŠ“å– - API æ–¹æ¡ˆ
ä½¿ç”¨ cookies ç›´æ¥è°ƒç”¨ API è·å–ç¬”è®°æ•°æ®

ä½¿ç”¨æ–¹æ³•:
    python fetch_note_api.py "ç¬”è®°URLæˆ–ID" --output /tmp/xhs_output
    python fetch_note_api.py "697d3a11000000002203a0c9" -o /tmp/xhs_output --ocr
"""

import argparse
import json
import os
import re
import requests
import hashlib
import time
import subprocess
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse, parse_qs


def load_cookies():
    """åŠ è½½ä¿å­˜çš„ cookies"""
    cookie_file = Path.home() / '.xiaohongshu-scraper' / 'cookies.json'
    if not cookie_file.exists():
        print(f"âŒ Cookie æ–‡ä»¶ä¸å­˜åœ¨: {cookie_file}")
        print("è¯·å…ˆè¿è¡Œç™»å½•è„šæœ¬è·å– cookies")
        return None
    
    with open(cookie_file, 'r') as f:
        cookies = json.load(f)
    
    # è½¬æ¢ä¸º cookie å­—ç¬¦ä¸²
    cookie_str = '; '.join([f"{c['name']}={c['value']}" for c in cookies])
    return cookie_str


def extract_note_id(url_or_id):
    """ä»å„ç§æ ¼å¼çš„é“¾æ¥ä¸­æå–ç¬”è®° ID"""
    # å¦‚æœå·²ç»æ˜¯çº¯ ID
    if re.match(r'^[a-f0-9]{24}$', url_or_id):
        return url_or_id
    
    # ä» URL ä¸­æå–
    patterns = [
        r'/explore/([a-f0-9]{24})',
        r'/discovery/item/([a-f0-9]{24})',
        r'/item/([a-f0-9]{24})',
        r'note_id=([a-f0-9]{24})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url_or_id)
        if match:
            return match.group(1)
    
    print(f"âŒ æ— æ³•ä»é“¾æ¥ä¸­æå–ç¬”è®° ID: {url_or_id}")
    return None


def get_xs_common():
    """ç”Ÿæˆ x-s ç­¾åï¼ˆç®€åŒ–ç‰ˆï¼Œå¯èƒ½éœ€è¦æ›´æ–°ï¼‰"""
    # å°çº¢ä¹¦çš„ç­¾åç®—æ³•æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œç”¨ç®€åŒ–æ–¹æ¡ˆ
    # å®é™…å¯èƒ½éœ€è¦é€†å‘ JS æˆ–ä½¿ç”¨ execjs
    return ""


def fetch_note_api(note_id, cookies):
    """é€šè¿‡ API è·å–ç¬”è®°æ•°æ®"""
    
    url = "https://edith.xiaohongshu.com/api/sns/web/v1/feed"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Content-Type': 'application/json;charset=UTF-8',
        'Origin': 'https://www.xiaohongshu.com',
        'Referer': f'https://www.xiaohongshu.com/explore/{note_id}',
        'Cookie': cookies,
    }
    
    payload = {
        "source_note_id": note_id,
        "image_formats": ["jpg", "webp", "avif"],
        "extra": {"need_body_topic": "1"}
    }
    
    try:
        resp = requests.post(url, headers=headers, json=payload, timeout=30)
        
        if resp.status_code == 200:
            data = resp.json()
            if data.get('success') or data.get('code') == 0:
                return data.get('data', {})
            else:
                print(f"API è¿”å›é”™è¯¯: {data}")
                return None
        else:
            print(f"HTTP é”™è¯¯: {resp.status_code}")
            print(resp.text[:500])
            return None
            
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None


def fetch_note_web(note_id, cookies):
    """é€šè¿‡ç½‘é¡µæ¥å£è·å–ç¬”è®°æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    
    url = f"https://www.xiaohongshu.com/explore/{note_id}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cookie': cookies,
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'same-origin',
    }
    
    try:
        resp = requests.get(url, headers=headers, timeout=30)
        
        if resp.status_code == 200:
            # ä» HTML ä¸­æå– JSON æ•°æ®
            # å°çº¢ä¹¦ä¼šåœ¨é¡µé¢ä¸­åµŒå…¥ __INITIAL_STATE__ æˆ–ç±»ä¼¼çš„æ•°æ®
            
            # æ–¹æ³•1: æŸ¥æ‰¾ window.__INITIAL_STATE__
            match = re.search(r'window\.__INITIAL_STATE__\s*=\s*({.+?})\s*</script>', resp.text, re.DOTALL)
            if match:
                try:
                    data = json.loads(match.group(1))
                    return data
                except:
                    pass
            
            # æ–¹æ³•2: æŸ¥æ‰¾ JSON-LD æ•°æ®
            match = re.search(r'<script type="application/ld\+json">(.+?)</script>', resp.text, re.DOTALL)
            if match:
                try:
                    data = json.loads(match.group(1))
                    return {'ld_json': data}
                except:
                    pass
            
            # æ–¹æ³•3: ç›´æ¥è§£æ HTML
            result = parse_html(resp.text)
            if result:
                return result
                
            return {'html': resp.text}
        else:
            print(f"HTTP é”™è¯¯: {resp.status_code}")
            return None
            
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return None


def parse_html(html):
    """ä» HTML ä¸­è§£æç¬”è®°æ•°æ®"""
    result = {}
    
    # æå–æ ‡é¢˜
    title_match = re.search(r'<title>([^<]+)</title>', html)
    if title_match:
        result['title'] = title_match.group(1).replace(' - å°çº¢ä¹¦', '').strip()
    
    # æå– meta æè¿°
    desc_match = re.search(r'<meta name="description" content="([^"]+)"', html)
    if desc_match:
        result['description'] = desc_match.group(1)
    
    # æå– og:image
    images = re.findall(r'<meta property="og:image" content="([^"]+)"', html)
    if images:
        result['images'] = images
    
    # æå–æ‰€æœ‰å›¾ç‰‡ URL
    all_images = re.findall(r'https://[^"\']+\.(?:jpg|jpeg|png|webp|avif)[^"\']*', html)
    # è¿‡æ»¤å¹¶å»é‡
    filtered_images = []
    for img in all_images:
        if 'sns-webpic' in img or 'ci.xiaohongshu.com' in img:
            # æ¸…ç† URL å‚æ•°
            clean_url = re.sub(r'\?.*$', '', img)
            if clean_url not in filtered_images:
                filtered_images.append(clean_url)
    
    if filtered_images:
        result['all_images'] = filtered_images
    
    return result if result else None


def download_image(url, save_path):
    """ä¸‹è½½å›¾ç‰‡"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Referer': 'https://www.xiaohongshu.com/'
    }
    
    try:
        resp = requests.get(url, headers=headers, timeout=30)
        if resp.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(resp.content)
            return True
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
    return False


def ocr_image_vision(image_path):
    """ä½¿ç”¨ macOS Vision æ¡†æ¶è¿›è¡Œ OCR"""
    
    # ä½¿ç”¨ Swift è„šæœ¬è°ƒç”¨ Vision
    swift_code = '''
import Vision
import AppKit

let imagePath = CommandLine.arguments[1]
guard let image = NSImage(contentsOfFile: imagePath),
      let cgImage = image.cgImage(forProposedRect: nil, context: nil, hints: nil) else {
    print("æ— æ³•åŠ è½½å›¾ç‰‡")
    exit(1)
}

let request = VNRecognizeTextRequest { request, error in
    guard let observations = request.results as? [VNRecognizedTextObservation] else { return }
    
    for observation in observations {
        if let topCandidate = observation.topCandidates(1).first {
            print(topCandidate.string)
        }
    }
}

request.recognitionLevel = .accurate
request.recognitionLanguages = ["zh-Hans", "zh-Hant", "en"]

let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
try? handler.perform([request])
'''
    
    # å†™å…¥ä¸´æ—¶ Swift æ–‡ä»¶
    swift_file = '/tmp/ocr_vision.swift'
    with open(swift_file, 'w') as f:
        f.write(swift_code)
    
    try:
        result = subprocess.run(
            ['swift', swift_file, image_path],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            print(f"OCR é”™è¯¯: {result.stderr}")
            return None
    except Exception as e:
        print(f"OCR å¤±è´¥: {e}")
        return None


def ocr_image_tesseract(image_path):
    """ä½¿ç”¨ Tesseract è¿›è¡Œ OCRï¼ˆå¤‡ç”¨ï¼‰"""
    try:
        result = subprocess.run(
            ['tesseract', image_path, 'stdout', '-l', 'chi_sim+eng'],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except FileNotFoundError:
        print("Tesseract æœªå®‰è£…ï¼Œè·³è¿‡")
    except Exception as e:
        print(f"Tesseract OCR å¤±è´¥: {e}")
    return None


def main():
    parser = argparse.ArgumentParser(description='å°çº¢ä¹¦ç¬”è®°æŠ“å– - API æ–¹æ¡ˆ')
    parser.add_argument('url', help='ç¬”è®° URL æˆ– ID')
    parser.add_argument('--output', '-o', help='è¾“å‡ºç›®å½•', default='/tmp/xhs_note')
    parser.add_argument('--ocr', action='store_true', help='å¯¹å›¾ç‰‡è¿›è¡Œ OCR')
    parser.add_argument('--ocr-engine', choices=['vision', 'tesseract'], default='vision', help='OCR å¼•æ“')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("å°çº¢ä¹¦ç¬”è®°æŠ“å– - API æ–¹æ¡ˆ")
    print("=" * 60)
    
    # åŠ è½½ cookies
    cookies = load_cookies()
    if not cookies:
        return
    
    print(f"âœ“ Cookies å·²åŠ è½½")
    
    # æå–ç¬”è®° ID
    note_id = extract_note_id(args.url)
    if not note_id:
        return
    
    print(f"âœ“ ç¬”è®° ID: {note_id}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    images_dir = output_dir / 'images'
    images_dir.mkdir(exist_ok=True)
    
    # å°è¯• API è·å–
    print("\næ­£åœ¨é€šè¿‡ API è·å–ç¬”è®°æ•°æ®...")
    data = fetch_note_api(note_id, cookies)
    
    if not data:
        print("API æ–¹å¼å¤±è´¥ï¼Œå°è¯•ç½‘é¡µæ–¹å¼...")
        data = fetch_note_web(note_id, cookies)
    
    if not data:
        print("âŒ æ— æ³•è·å–ç¬”è®°æ•°æ®")
        return
    
    # ä¿å­˜åŸå§‹æ•°æ®
    raw_file = output_dir / 'raw_data.json'
    with open(raw_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ“ åŸå§‹æ•°æ®å·²ä¿å­˜: {raw_file}")
    
    # è§£æç¬”è®°æ•°æ®
    result = {
        'note_id': note_id,
        'fetch_time': datetime.now().isoformat(),
        'title': '',
        'author': '',
        'content': '',
        'images': [],
        'local_images': [],
        'ocr_results': []
    }
    
    # ä»ä¸åŒæ•°æ®ç»“æ„ä¸­æå–ä¿¡æ¯
    if 'items' in data and data['items']:
        note_data = data['items'][0].get('note_card', {})
        result['title'] = note_data.get('title', '')
        result['content'] = note_data.get('desc', '')
        result['author'] = note_data.get('user', {}).get('nickname', '')
        
        # æå–å›¾ç‰‡
        image_list = note_data.get('image_list', [])
        for img in image_list:
            # ä¼˜å…ˆä½¿ç”¨é«˜æ¸…å›¾
            url = img.get('url_default') or img.get('url') or img.get('url_pre', '')
            if url and url not in result['images']:
                result['images'].append(url)
    
    elif 'title' in data:
        result['title'] = data.get('title', '')
        result['content'] = data.get('description', '')
        result['images'] = data.get('all_images', data.get('images', []))
    
    elif 'html' in data:
        # éœ€è¦è¿›ä¸€æ­¥è§£æ HTML
        parsed = parse_html(data['html'])
        if parsed:
            result['title'] = parsed.get('title', '')
            result['content'] = parsed.get('description', '')
            result['images'] = parsed.get('all_images', [])
    
    print(f"\nğŸ“ æ ‡é¢˜: {result['title']}")
    print(f"ğŸ‘¤ ä½œè€…: {result['author']}")
    print(f"ğŸ“„ å†…å®¹: {result['content'][:100]}..." if len(result['content']) > 100 else f"ğŸ“„ å†…å®¹: {result['content']}")
    print(f"ğŸ–¼ï¸  å›¾ç‰‡: {len(result['images'])} å¼ ")
    
    # ä¸‹è½½å›¾ç‰‡
    if result['images']:
        print(f"\næ­£åœ¨ä¸‹è½½ {len(result['images'])} å¼ å›¾ç‰‡...")
        for i, img_url in enumerate(result['images']):
            ext = '.jpg'
            if '.png' in img_url:
                ext = '.png'
            elif '.webp' in img_url:
                ext = '.webp'
            
            filename = f"image_{i+1:02d}{ext}"
            save_path = images_dir / filename
            
            if download_image(img_url, str(save_path)):
                result['local_images'].append(str(save_path))
                print(f"  âœ“ {filename}")
            else:
                print(f"  âœ— {filename}")
    
    # OCR
    if args.ocr and result['local_images']:
        print(f"\næ­£åœ¨è¿›è¡Œ OCR è¯†åˆ« ({args.ocr_engine})...")
        
        for img_path in result['local_images']:
            print(f"  å¤„ç†: {Path(img_path).name}")
            
            if args.ocr_engine == 'vision':
                text = ocr_image_vision(img_path)
            else:
                text = ocr_image_tesseract(img_path)
            
            if text:
                result['ocr_results'].append({
                    'image': img_path,
                    'text': text
                })
                print(f"    è¯†åˆ«åˆ° {len(text)} å­—ç¬¦")
            else:
                print(f"    æœªè¯†åˆ«åˆ°æ–‡å­—")
    
    # ä¿å­˜ç»“æœ
    result_file = output_dir / 'note.json'
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆ Markdown
    md_content = f"""# {result['title']}

**ä½œè€…**: {result['author']}  
**æŠ“å–æ—¶é—´**: {result['fetch_time']}  
**ç¬”è®°ID**: {result['note_id']}

## æ­£æ–‡å†…å®¹

{result['content']}

## å›¾ç‰‡

"""
    
    for i, img_path in enumerate(result['local_images']):
        md_content += f"### å›¾ç‰‡ {i+1}\n\n"
        md_content += f"![image_{i+1}](images/{Path(img_path).name})\n\n"
        
        # æ·»åŠ  OCR ç»“æœ
        for ocr in result['ocr_results']:
            if ocr['image'] == img_path:
                md_content += f"**OCR è¯†åˆ«æ–‡å­—:**\n\n```\n{ocr['text']}\n```\n\n"
                break
    
    md_file = output_dir / 'note.md'
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"\n" + "=" * 60)
    print("âœ… æŠ“å–å®Œæˆ!")
    print("=" * 60)
    print(f"\nè¾“å‡ºç›®å½•: {output_dir}")
    print(f"  - note.json    (ç»“æ„åŒ–æ•°æ®)")
    print(f"  - note.md      (Markdown æ–‡æ¡£)")
    print(f"  - raw_data.json (åŸå§‹ API æ•°æ®)")
    print(f"  - images/      ({len(result['local_images'])} å¼ å›¾ç‰‡)")
    if result['ocr_results']:
        print(f"  - OCR ç»“æœå·²åŒ…å«åœ¨ JSON å’Œ MD ä¸­")


if __name__ == '__main__':
    main()
