---
name: Storage
description: Choose and architect storage systems for applications with the right tradeoffs.
metadata: {"clawdbot":{"emoji":"ðŸ’¾","os":["linux","darwin","win32"]}}
---

## Object vs Block vs File

- Object storage (S3, R2, GCS) for immutable blobs: images, videos, backups, logs â€” cheap, scales infinitely, but no partial updates
- Block storage (EBS, Persistent Disks) for databases and apps needing filesystem semantics â€” faster, but tied to single instance
- Network file systems (NFS, EFS) when multiple instances need shared filesystem access â€” convenient but latency and cost add up
- Default to object storage for user uploads â€” block storage for database files only

## When SQL vs NoSQL

- SQL when you need joins, transactions, or complex queries â€” fighting against NoSQL for relational data wastes months
- Document stores (MongoDB, Firestore) for nested/variable schemas where you always fetch the whole document
- Key-value (Redis, DynamoDB) for simple lookups by ID at massive scale â€” not for complex queries
- Time-series databases (InfluxDB, TimescaleDB) for metrics with timestamp-based queries â€” regular SQL struggles with retention policies
- Start with PostgreSQL unless you have a specific reason not to â€” it handles JSON, full-text search, and scales further than most assume

## Local vs Cloud Storage

- Local disk for ephemeral data: temp files, build artifacts, caches â€” assume it disappears on restart
- Cloud storage for anything that must survive instance termination â€” never store user data only on local disk
- Local SSD for databases in production â€” network-attached storage adds latency to every query
- Hybrid: local cache in front of cloud storage for frequently accessed files

## CDN Patterns

- Put CDN in front of static assets always â€” origin requests are slower and more expensive
- Set long cache TTLs with versioned URLs (`style.abc123.css`) â€” cache invalidation is slow and unreliable
- CDN for dynamic content only if latency matters more than freshness â€” adds complexity for marginal gains
- Edge caching for API responses works but cache keys get tricky â€” start simple, add only when needed

## Upload Handling

- Never accept uploads directly to app server disk in production â€” use presigned URLs to cloud storage
- Set file size limits at load balancer level, not just application â€” prevents memory exhaustion attacks
- Generate unique keys for uploads (UUIDs) â€” user-provided filenames cause collisions and path traversal risks
- Validate file types by content (magic bytes), not extension â€” extensions are trivially spoofed

## Data Locality

- Keep compute and storage in same region â€” cross-region data transfer adds latency and cost
- Replicate data to regions where users are, not where developers are
- Multi-region storage adds complexity â€” single region with backups elsewhere usually sufficient
- Database read replicas in user regions for read-heavy workloads

## Retention and Lifecycle

- Define retention policy before storing data â€” "keep everything" becomes expensive and legally risky
- Automate deletion of temporary data â€” manual cleanup never happens consistently
- Tiered storage for aging data: hot â†’ warm â†’ cold â†’ archive â€” but check retrieval costs before archiving
- Separate storage for logs vs business data â€” different retention, different compliance requirements

## Cost Traps

- Egress fees dominate cloud storage costs â€” calculate before choosing provider
- Many small files cost more than few large files â€” batch small writes when possible
- Minimum storage duration on cold tiers â€” early deletion still charges full period
- API request costs matter at scale â€” millions of LIST operations add up

## Backup Strategy

- 3-2-1 rule: 3 copies, 2 different media types, 1 offsite â€” cloud counts as one location
- Test restores regularly â€” untested backups are not backups
- Point-in-time recovery for databases â€” daily snapshots lose a day of data
- Version important files â€” deletion or corruption often discovered late
