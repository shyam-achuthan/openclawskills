#!/usr/bin/env python3
"""
Raon OS â€” RAG Pipeline (Chunker + Embedder + Retriever)
Python 3.9+ compatible (no 3.10+ union type hints)

ì„ë² ë”©/LLM: raon_llm.py ë²”ìš© í´ë¼ì´ì–¸íŠ¸ ìœ„ì„
  â†’ OpenRouter / Gemini / OpenAI / Ollama ìë™ê°ì§€
  â†’ ì„ë² ë”© ì—†ìœ¼ë©´ BM25 keyword ê²€ìƒ‰ í´ë°±

Usage:
    python3 rag_pipeline.py ingest --data-dir ../eval_data
    python3 rag_pipeline.py search --query "TIPS í•©ê²©í•˜ë ¤ë©´?"
    python3 rag_pipeline.py eval
"""

from __future__ import annotations  # Python 3.9 compatibility
import argparse
import json
import math
import os
import sys
import time
from pathlib import Path

# raon_llm: ë²”ìš© LLM/ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (.env ìë™ë¡œë“œ í¬í•¨)
from raon_llm import (
    chat as _raon_chat,
    embed as _raon_embed,
    cosine_sim as cosine_similarity,
    detect_embed_provider,
    prompt_to_messages,
)

BASE_DIR = Path(__file__).resolve().parent.parent
EVAL_DATA_DIR = BASE_DIR / "eval_data"
VECTOR_STORE_PATH = BASE_DIR / "vector_store.json"

EMBED_MODEL = os.environ.get("EMBED_MODEL", "bge-m3")  # Ollama í´ë°±ìš© íŒíŠ¸


# â”€â”€â”€ BM25 (simple keyword scoring) â”€â”€â”€

# kiwipiepy í˜•íƒœì†Œ ë¶„ì„ê¸° (lazy init)
_kiwi_instance = None
_kiwi_available = None

def _get_kiwi():
    """kiwipiepy Kiwi ì¸ìŠ¤í„´ìŠ¤ë¥¼ lazy ë¡œë”©."""
    global _kiwi_instance, _kiwi_available
    if _kiwi_available is None:
        try:
            from kiwipiepy import Kiwi
            _kiwi_instance = Kiwi()
            _kiwi_available = True
        except ImportError:
            _kiwi_available = False
    return _kiwi_instance if _kiwi_available else None


def _tokenize_kiwi(text: str) -> list[str]:
    """kiwipiepy í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í† í¬ë‚˜ì´ì €."""
    kiwi = _get_kiwi()
    if kiwi is None:
        return _tokenize_bigram(text)
    tokens = []
    for token in kiwi.tokenize(text):
        form = token.form.lower()
        # ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬ë§Œ: NNG(ì¼ë°˜ëª…ì‚¬), NNP(ê³ ìœ ëª…ì‚¬), VV(ë™ì‚¬), VA(í˜•ìš©ì‚¬),
        # SL(ì™¸êµ­ì–´), SN(ìˆ«ì), NNB(ì˜ì¡´ëª…ì‚¬), XR(ì–´ê·¼)
        if token.tag in ('NNG', 'NNP', 'VV', 'VA', 'SL', 'SN', 'NNB', 'XR', 'MAG'):
            tokens.append(form)
    return tokens


def _tokenize_bigram(text: str) -> list[str]:
    """Fallback: character n-gram + word tokenizer for Korean/English."""
    import re
    words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text.lower())
    bigrams = []
    for w in words:
        if any('\uac00' <= c <= '\ud7a3' for c in w):
            for i in range(len(w) - 1):
                bigrams.append(w[i:i+2])
    return words + bigrams


def _tokenize(text: str) -> list[str]:
    """Korean/English tokenizer: kiwipiepy if available, else bigram fallback."""
    return _tokenize_kiwi(text)


def _build_bm25_index(store: list[dict]) -> dict:
    """Build inverted index for BM25 scoring."""
    import math
    doc_count = len(store)
    # doc frequency per term
    df = {}
    doc_tokens = []
    avg_dl = 0
    for item in store:
        tokens = _tokenize(item["text"])
        doc_tokens.append(tokens)
        avg_dl += len(tokens)
        seen = set()
        for t in tokens:
            if t not in seen:
                df[t] = df.get(t, 0) + 1
                seen.add(t)
    avg_dl = avg_dl / max(doc_count, 1)
    return {"df": df, "doc_tokens": doc_tokens, "avg_dl": avg_dl, "N": doc_count}


def bm25_score(query: str, doc_idx: int, index: dict, k1: float = 1.5, b: float = 0.75) -> float:
    """BM25 score for a single document."""
    import math
    q_tokens = _tokenize(query)
    doc_tokens = index["doc_tokens"][doc_idx]
    dl = len(doc_tokens)
    avg_dl = index["avg_dl"]
    N = index["N"]
    df = index["df"]

    # Term frequency in doc
    tf_map = {}
    for t in doc_tokens:
        tf_map[t] = tf_map.get(t, 0) + 1

    score = 0.0
    for qt in q_tokens:
        if qt not in tf_map:
            continue
        tf = tf_map[qt]
        doc_freq = df.get(qt, 0)
        idf = math.log((N - doc_freq + 0.5) / (doc_freq + 0.5) + 1)
        tf_norm = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * dl / max(avg_dl, 1)))
        score += idf * tf_norm
    return score


def hybrid_search(query: str, top_k: int = 3, model: str = EMBED_MODEL,
                  store: list[dict] | None = None,
                  vector_weight: float = 0.6, bm25_weight: float = 0.4) -> list[dict]:
    """Hybrid search: vector cosine + BM25."""
    if store is None:
        store = load_vector_store()
    if not store:
        return []

    # BM25 ì¸ë±ìŠ¤ (í•­ìƒ ë¹Œë“œ)
    bm25_index = _build_bm25_index(store)
    raw_bm25 = [bm25_score(query, i, bm25_index) for i in range(len(store))]
    max_bm25 = max(raw_bm25) if raw_bm25 else 1.0
    bm25_scores = [s / max_bm25 if max_bm25 > 0 else 0.0 for s in raw_bm25]

    # Vector scores (í´ë°±: ë²¡í„° ì—†ìœ¼ë©´ BM25 ì „ìš©)
    q_emb = get_embedding(query, model)
    if not q_emb:
        # BM25 ì „ìš© ëª¨ë“œ
        scored = []
        for i, item in enumerate(store):
            scored.append({**item, "score": bm25_scores[i], "bm25_score": bm25_scores[i]})
        scored.sort(key=lambda x: x["score"], reverse=True)
        return scored[:top_k]

    vector_scores = []
    for item in store:
        sim = cosine_similarity(q_emb, item.get("embedding", []))
        vector_scores.append(sim)

    # Combine
    scored = []
    for i, item in enumerate(store):
        combined = vector_weight * vector_scores[i] + bm25_weight * bm25_scores[i]
        scored.append({**item, "score": combined,
                       "vector_score": vector_scores[i],
                       "bm25_score": bm25_scores[i]})

    scored.sort(key=lambda x: x["score"], reverse=True)
    return scored[:top_k]


# â”€â”€â”€ Chunker â”€â”€â”€

def chunk_jsonl_entry(entry: dict) -> list[dict]:
    """JSONL í•­ëª©ì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ chunk(ë“¤)ë¡œ ë³€í™˜."""
    entry_type = entry.get("type", "unknown")
    chunks = []
    
    # ë©”íƒ€ë°ì´í„° (ê²€ìƒ‰ì— ì•ˆ ì“°ì´ì§€ë§Œ ê²°ê³¼ í‘œì‹œìš©)
    meta = {
        "type": entry_type,
        "source": entry.get("source", ""),
        "tags": entry.get("tags", []),
    }
    
    if entry_type == "success_case":
        # ì„±ê³µì‚¬ë¡€: title + summary + tipsë¥¼ í•˜ë‚˜ì˜ chunkë¡œ
        text_parts = []
        if entry.get("title"):
            text_parts.append(entry["title"])
        if entry.get("program"):
            text_parts.append(f"í”„ë¡œê·¸ë¨: {entry['program']}")
        if entry.get("year"):
            text_parts.append(f"ì—°ë„: {entry['year']}")
        if entry.get("summary"):
            text_parts.append(entry["summary"])
        if entry.get("tips"):
            text_parts.append(f"íŒ: {entry['tips']}")
        
        meta["title"] = entry.get("title", "")
        meta["program"] = entry.get("program", "")
        chunks.append({"text": "\n".join(text_parts), "meta": meta})
    
    elif entry_type == "criteria":
        text_parts = []
        if entry.get("category"):
            text_parts.append(f"ì¹´í…Œê³ ë¦¬: {entry['category']}")
        if entry.get("criteria"):
            text_parts.append(f"ì‹¬ì‚¬ê¸°ì¤€: {entry['criteria']}")
        if entry.get("description"):
            text_parts.append(entry["description"])
        if entry.get("weight"):
            text_parts.append(f"ê°€ì¤‘ì¹˜: {entry['weight']}")
        
        meta["category"] = entry.get("category", "")
        meta["criteria"] = entry.get("criteria", "")
        chunks.append({"text": "\n".join(text_parts), "meta": meta})
    
    elif entry_type == "gov_program":
        text_parts = []
        if entry.get("program"):
            text_parts.append(f"ì‚¬ì—…ëª…: {entry['program']}")
        if entry.get("category"):
            text_parts.append(f"ë¶„ë¥˜: {entry['category']}")
        if entry.get("year"):
            text_parts.append(f"ì—°ë„: {entry['year']}")
        if entry.get("description"):
            text_parts.append(entry["description"])
        if entry.get("budget"):
            text_parts.append(f"ì˜ˆì‚°: {entry['budget']}")
        
        meta["program"] = entry.get("program", "")
        meta["year"] = entry.get("year", "")
        chunks.append({"text": "\n".join(text_parts), "meta": meta})
    
    elif entry_type == "vc_investment":
        text_parts = []
        if entry.get("title"):
            text_parts.append(entry["title"])
        if entry.get("category"):
            text_parts.append(f"ë¶„ë¥˜: {entry['category']}")
        if entry.get("year"):
            text_parts.append(f"ì—°ë„: {entry['year']}")
        if entry.get("data"):
            text_parts.append(entry["data"])
        
        meta["title"] = entry.get("title", "")
        meta["year"] = entry.get("year", "")
        chunks.append({"text": "\n".join(text_parts), "meta": meta})
    
    else:
        # Fallback: ëª¨ë“  string í•„ë“œë¥¼ í•©ì¹¨
        text_parts = [f"{k}: {v}" for k, v in entry.items() 
                      if isinstance(v, str) and k not in ("source", "type")]
        chunks.append({"text": "\n".join(text_parts), "meta": meta})
    
    return chunks


def load_all_chunks(data_dir: Path) -> list[dict]:
    """eval_data/ ë‚´ ëª¨ë“  JSONL íŒŒì¼ì—ì„œ chunk ì¶”ì¶œ."""
    all_chunks = []
    for jsonl_file in sorted(data_dir.glob("*.jsonl")):
        count = 0
        for line in jsonl_file.read_text().strip().split("\n"):
            line = line.strip()
            if not line:
                continue
            entry = json.loads(line)
            chunks = chunk_jsonl_entry(entry)
            for c in chunks:
                c["meta"]["file"] = jsonl_file.name
            all_chunks.extend(chunks)
            count += 1
        print(f"  ğŸ“„ {jsonl_file.name}: {count}ê±´ â†’ {len([c for c in all_chunks if c['meta'].get('file') == jsonl_file.name])} chunks")
    return all_chunks


# â”€â”€â”€ Embedder (raon_llm ìœ„ì„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤ì œ êµ¬í˜„ì€ raon_llm.embed()ê°€ ë‹´ë‹¹:
#   Gemini text-embedding-004 â†’ OpenAI text-embedding-3-small â†’ Ollama â†’ []

def get_embedding(text: str, model: str = EMBED_MODEL) -> list:
    """raon_llm.embed()ë¡œ ìœ„ì„. ì‹¤íŒ¨ ì‹œ [] (BM25 í´ë°±)."""
    return _raon_embed(text)


def get_embeddings_batch(texts: list, model: str = EMBED_MODEL) -> list:
    """ë°°ì¹˜ ì„ë² ë”©. raon_llm.embed() ìˆœì°¨ í˜¸ì¶œ."""
    return [_raon_embed(t) for t in texts]


def embed_chunks(chunks: list[dict], model: str = EMBED_MODEL, batch_size: int = 50) -> list[dict]:
    """ëª¨ë“  chunkì— ì„ë² ë”© ë²¡í„° ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬)."""
    total = len(chunks)
    for batch_start in range(0, total, batch_size):
        batch = chunks[batch_start:batch_start + batch_size]
        texts = [c["text"] for c in batch]
        if batch_start == 0 or (batch_start + batch_size) % 500 == 0:
            print(f"  ğŸ”„ ì„ë² ë”© {batch_start + 1}â€“{min(batch_start + batch_size, total)}/{total}...")
        embeddings = get_embeddings_batch(texts, model)
        for chunk, emb in zip(batch, embeddings):
            chunk["embedding"] = emb
    # ì„ë² ë”© ì‹¤íŒ¨í•œ ê²ƒ ì œê±°
    valid = [c for c in chunks if c.get("embedding")]
    print(f"  âœ… ì„ë² ë”© ì™„ë£Œ: {len(valid)}/{total} ì„±ê³µ")
    return valid


# â”€â”€â”€ Vector Store â”€â”€â”€

def save_vector_store(chunks: list[dict], path: Path = VECTOR_STORE_PATH):
    """ë²¡í„° ì €ì¥ì†Œë¥¼ JSONìœ¼ë¡œ ì €ì¥."""
    store = []
    for c in chunks:
        store.append({
            "text": c["text"],
            "meta": c["meta"],
            "embedding": c["embedding"],
        })
    path.write_text(json.dumps(store, ensure_ascii=False))
    size_mb = path.stat().st_size / 1024 / 1024
    print(f"  ğŸ’¾ ì €ì¥: {path.name} ({len(store)}ê±´, {size_mb:.1f}MB)")


def load_vector_store(path: Path = VECTOR_STORE_PATH) -> list[dict]:
    """ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ."""
    if not path.exists():
        return []
    return json.loads(path.read_text())


# â”€â”€â”€ Retriever â”€â”€â”€

# cosine_similarity: raon_llm.cosine_sim ìœ¼ë¡œ import (ìƒë‹¨ ì°¸ì¡°)


def rerank(query: str, candidates: list, top_k: int = 3,
           llm_model: str = "") -> list:
    """LLM ê¸°ë°˜ reranker: í›„ë³´ ë¬¸ì„œë“¤ì„ ì¿¼ë¦¬ ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì¬ì •ë ¬.
    llm_model íŒŒë¼ë¯¸í„°ëŠ” í•˜ìœ„ í˜¸í™˜ìš© (raon_llmì´ ìë™ ê°ì§€)."""
    if not candidates:
        return []
    if len(candidates) <= top_k:
        return candidates

    # í›„ë³´ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ë²ˆí˜¸ ë§¤ê¸°ê¸°)
    doc_list = []
    for i, c in enumerate(candidates):
        preview = c["text"][:300].replace("\n", " ")
        doc_list.append(f"[{i}] {preview}")
    docs_text = "\n".join(doc_list)

    prompt = (
        f"ì¿¼ë¦¬: {query}\n\n"
        f"ë‹¤ìŒ ë¬¸ì„œë“¤ ì¤‘ ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ìˆœì„œëŒ€ë¡œ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ë‚˜ì—´í•´. "
        f"ìƒìœ„ {top_k}ê°œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ë²ˆí˜¸ë§Œ ì¶œë ¥í•´. ì„¤ëª… ì—†ì´ ë²ˆí˜¸ë§Œ.\n\n"
        f"{docs_text}\n\n"
        f"ë‹µë³€ (ë²ˆí˜¸ë§Œ, ì˜ˆ: 3,1,7):"
    )

    response_text = _raon_chat(prompt_to_messages(prompt)) or ""
    if not response_text:
        print(f"  âš ï¸ Rerank LLM í˜¸ì¶œ ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ë°˜í™˜")
        return candidates[:top_k]

    # ì‘ë‹µì—ì„œ ìˆ«ì ì¶”ì¶œ
    import re
    numbers = re.findall(r'\d+', response_text)
    seen = set()
    reranked = []
    for n in numbers:
        idx = int(n)
        if 0 <= idx < len(candidates) and idx not in seen:
            seen.add(idx)
            reranked.append(candidates[idx])
        if len(reranked) >= top_k:
            break

    # LLMì´ ì¶©ë¶„í•œ ê²°ê³¼ë¥¼ ëª» ì¤¬ìœ¼ë©´ ì›ë³¸ì—ì„œ ì±„ì›€
    if len(reranked) < top_k:
        for c in candidates:
            if c not in reranked:
                reranked.append(c)
            if len(reranked) >= top_k:
                break

    return reranked


def search(query: str, top_k: int = 3, model: str = EMBED_MODEL,
           store: list[dict] | None = None) -> list[dict]:
    """ì¿¼ë¦¬ë¡œ top-k ê²€ìƒ‰."""
    if store is None:
        store = load_vector_store()
    if not store:
        print("âŒ ë²¡í„° ì €ì¥ì†Œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ingestë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return []
    
    q_emb = get_embedding(query, model)
    if not q_emb:
        return []
    
    scored = []
    for item in store:
        sim = cosine_similarity(q_emb, item["embedding"])
        scored.append({**item, "score": sim})
    
    scored.sort(key=lambda x: x["score"], reverse=True)
    return scored[:top_k]


# â”€â”€â”€ Commands â”€â”€â”€

def cmd_ingest(args):
    """JSONL ë°ì´í„°ë¥¼ chunk â†’ embed â†’ ì €ì¥."""
    data_dir = Path(args.data_dir) if args.data_dir else EVAL_DATA_DIR
    model = args.model or EMBED_MODEL

    print(f"ğŸš€ RAG ì¸ì œìŠ¤íŠ¸ ì‹œì‘")
    print(f"   ë°ì´í„°: {data_dir}")
    print(f"   ëª¨ë¸: {model}")

    # 1) Chunk
    print(f"\nğŸ“¦ 1ë‹¨ê³„: ì²­í‚¹")
    chunks = load_all_chunks(data_dir)
    print(f"   ì´ {len(chunks)} chunks")

    # ì„ë² ë”© ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨ (raon_llm ìë™ê°ì§€)
    embed_prov = detect_embed_provider()

    if embed_prov == "none":
        print(f"\nâš ï¸ ì„ë² ë”© í”„ë¡œë°”ì´ë” ì—†ìŒ â€” BM25 keyword ê²€ìƒ‰ ëª¨ë“œë¡œ ë™ì‘")
        print(f"   ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©: ~/.openclaw/.env ì— GEMINI_API_KEY ë˜ëŠ” OPENAI_API_KEY ì¶”ê°€")
        # ì„ë² ë”© ì—†ì´ ì²­í¬ ì €ì¥ (BM25 ì „ìš©)
        for c in chunks:
            c["embedding"] = []
        print(f"\nğŸ’¾ 2ë‹¨ê³„: ì €ì¥ (BM25 ì „ìš©)")
        save_vector_store(chunks)
        print(f"\nâœ… ì¸ì œìŠ¤íŠ¸ ì™„ë£Œ (BM25 ëª¨ë“œ)! {len(chunks)}ê±´ ì €ì¥")
        return

    # 2) Embed
    print(f"\nğŸ§  2ë‹¨ê³„: ì„ë² ë”©")
    print(f"   ì—”ì§„: {embed_prov}")
    chunks = embed_chunks(chunks, model)

    # 3) Save
    print(f"\nğŸ’¾ 3ë‹¨ê³„: ì €ì¥")
    save_vector_store(chunks)

    print(f"\nâœ… ì¸ì œìŠ¤íŠ¸ ì™„ë£Œ! {len(chunks)}ê±´ ë²¡í„° ì €ì¥ì†Œ ìƒì„±")


def cmd_search(args):
    """ì¿¼ë¦¬ ê²€ìƒ‰."""
    use_rerank = getattr(args, 'rerank', False)
    model = args.model or EMBED_MODEL

    if use_rerank:
        # Top-10 í›„ë³´ â†’ LLM rerank â†’ Top-k
        candidates = hybrid_search(args.query, top_k=10, model=model)
        print(f"\nğŸ” ì¿¼ë¦¬: {args.query} (rerank: ON, í›„ë³´ {len(candidates)}ê±´)")
        results = rerank(args.query, candidates, top_k=args.top_k)
    else:
        results = hybrid_search(args.query, top_k=args.top_k, model=model)
        print(f"\nğŸ” ì¿¼ë¦¬: {args.query}")

    print(f"   ê²°ê³¼: {len(results)}ê±´\n")
    for i, r in enumerate(results, 1):
        score_str = f"score: {r['score']:.4f}" if 'score' in r else ""
        print(f"  [{i}] ({score_str}) [{r['meta'].get('type','')}]")
        print(f"      {r['text'][:150]}...")
        print()


def cmd_eval(args):
    """10ê°œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ í’ˆì§ˆ ê²€ì¦."""
    queries = [
        "TIPS í•©ê²©í•˜ë ¤ë©´ ì–´ë–»ê²Œ ì¤€ë¹„í•´ì•¼ í•´?",
        "ì´ˆê¸°ì°½ì—…íŒ¨í‚¤ì§€ ì‹¬ì‚¬ê¸°ì¤€ì€?",
        "ì˜ˆë¹„ì°½ì—…íŒ¨í‚¤ì§€ í•©ê²© í›„ê¸° ì•Œë ¤ì¤˜",
        "2026ë…„ ì •ë¶€ ì°½ì—…ì§€ì›ì‚¬ì—… ë­ê°€ ìˆì–´?",
        "ë²¤ì²˜íˆ¬ì í˜„í™© ì•Œë ¤ì¤˜",
        "TIPS ìš´ì˜ì‚¬ë³„ ì°¨ì´ì ì€?",
        "ì°½ì—…ë„ì•½íŒ¨í‚¤ì§€ ì‹ ì²­ ìê²©ì€?",
        "ì²­ë…„ì°½ì—…ì‚¬ê´€í•™êµ í•©ê²© ë…¸í•˜ìš°",
        "ì‹œë¦¬ì¦ˆA íˆ¬ì ë°›ìœ¼ë ¤ë©´?",
        "ì •ë¶€ì§€ì›ì‚¬ì—… ê°€ì  ë°›ëŠ” ë°©ë²•",
    ]
    
    model = args.model or EMBED_MODEL
    store = load_vector_store()
    if not store:
        print("âŒ ë²¡í„° ì €ì¥ì†Œ ì—†ìŒ. ë¨¼ì € ingest ì‹¤í–‰ í•„ìš”.")
        return
    
    print(f"ğŸ“Š RAG í’ˆì§ˆ ê²€ì¦ (ëª¨ë¸: {model}, ì €ì¥ì†Œ: {len(store)}ê±´)")
    print(f"{'='*70}\n")
    
    results_log = []
    
    for qi, query in enumerate(queries, 1):
        results = hybrid_search(query, top_k=3, model=model, store=store)
        print(f"Q{qi}: {query}")
        
        query_results = []
        for i, r in enumerate(results, 1):
            rtype = r["meta"].get("type", "?")
            score = r["score"]
            title = r["meta"].get("title", r["meta"].get("criteria", r["meta"].get("program", "")))
            text_preview = r["text"][:120].replace("\n", " ")
            
            # ìë™ relevance íŒì • (score ê¸°ë°˜)
            if score >= 0.7:
                relevance = "HIGH"
            elif score >= 0.5:
                relevance = "MED"
            else:
                relevance = "LOW"
            
            print(f"  #{i} [{relevance}] (sim={score:.4f}) [{rtype}] {title}")
            print(f"      {text_preview}...")
            
            query_results.append({
                "rank": i,
                "score": round(score, 4),
                "relevance": relevance,
                "type": rtype,
                "title": title,
                "text_preview": text_preview,
            })
        
        print()
        results_log.append({"query": query, "results": query_results})
    
    # ê²°ê³¼ ì €ì¥
    report_path = BASE_DIR / "rag_eval_report.json"
    report_path.write_text(json.dumps(results_log, ensure_ascii=False, indent=2))
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {report_path.name}")
    
    # ìš”ì•½
    print(f"\n{'='*70}")
    print("ğŸ“ˆ ìš”ì•½")
    high_count = sum(1 for q in results_log for r in q["results"] if r["relevance"] == "HIGH")
    med_count = sum(1 for q in results_log for r in q["results"] if r["relevance"] == "MED")
    low_count = sum(1 for q in results_log for r in q["results"] if r["relevance"] == "LOW")
    total_results = sum(len(q["results"]) for q in results_log)
    print(f"  HIGH: {high_count}/{total_results}, MED: {med_count}/{total_results}, LOW: {low_count}/{total_results}")
    
    # ì¿¼ë¦¬ë³„ top-1 relevance
    top1_high = sum(1 for q in results_log if q["results"] and q["results"][0]["relevance"] == "HIGH")
    print(f"  Top-1 HIGH ë¹„ìœ¨: {top1_high}/{len(queries)} ({top1_high/len(queries)*100:.0f}%)")


def main():
    parser = argparse.ArgumentParser(description="Raon OS RAG Pipeline")
    sub = parser.add_subparsers(dest="command")
    
    p_ingest = sub.add_parser("ingest", help="JSONL â†’ chunk â†’ embed â†’ ì €ì¥")
    p_ingest.add_argument("--data-dir", default=str(EVAL_DATA_DIR))
    p_ingest.add_argument("--model", default=EMBED_MODEL)
    
    p_search = sub.add_parser("search", help="ì¿¼ë¦¬ ê²€ìƒ‰")
    p_search.add_argument("--query", "-q", required=True)
    p_search.add_argument("--top-k", "-k", type=int, default=3)
    p_search.add_argument("--model", default=EMBED_MODEL)
    p_search.add_argument("--rerank", action="store_true", default=False,
                          help="LLM rerankerë¡œ Top-10â†’Top-k ì •ë°€ë„ í–¥ìƒ")
    
    p_eval = sub.add_parser("eval", help="10ê°œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ í’ˆì§ˆ ê²€ì¦")
    p_eval.add_argument("--model", default=EMBED_MODEL)
    
    args = parser.parse_args()
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    {"ingest": cmd_ingest, "search": cmd_search, "eval": cmd_eval}[args.command](args)


if __name__ == "__main__":
    main()
