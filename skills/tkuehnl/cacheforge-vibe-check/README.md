# ğŸ­ Vibe Check

> Audit code for "vibe coding sins" â€” patterns that indicate AI-generated code was accepted without proper human review.

![Vibe Score](https://img.shields.io/badge/vibe--score-73%2F100-yellow)

**Vibe Check** scans your Python, TypeScript, and JavaScript code for 8 categories of "vibe coding" sins â€” the telltale patterns of AI-generated code that was accepted without proper review. It produces a scored report card with specific line-level findings and optional fix suggestions.

## Features

- ğŸ” **8 sin categories** â€” error handling, duplication, dead code, input validation, magic values, test coverage, naming, security
- ğŸ“Š **Scored report card** â€” 0-100 score with letter grades (A-F)
- ğŸ“ **Flexible input** â€” single file, directory (recursive), or git diff
- ğŸ”§ **Fix mode** â€” generates unified diff patches for each finding
- ğŸ·ï¸ **README badge** â€” copy-pasteable shields.io badge for your repo
- ğŸ¤– **LLM-powered** â€” uses Claude or GPT for deep analysis, with heuristic fallback
- ğŸ’¬ **Discord v2-ready delivery** â€” compact first response + quick follow-up actions for OpenClaw Discord channels

## Quick Start

```bash
# Install from ClawHub
clawhub install cacheforge/vibe-check

# Analyze a directory
./scripts/vibe-check.sh src/

# Analyze a single file
./scripts/vibe-check.sh app.py

# Check your last 3 commits
./scripts/vibe-check.sh --diff HEAD~3

# With fix suggestions
./scripts/vibe-check.sh --fix src/

# Save report to file
./scripts/vibe-check.sh --fix --output report.md src/
```

## OpenClaw Discord v2 Ready

Tested against OpenClaw Discord channel capabilities documented for v2026.2.14+:
- Sends a compact first response (score + top findings), then expands on demand
- Avoids dumping full wide tables into the first Discord message
- Supports component-style follow-ups when available (`Show Top Findings`, `Show Fix Suggestions`, `Run Diff Mode`)

## Requirements

- **bash** (4.0+)
- **python3** (stdlib only â€” no pip installs)
- **curl** (for LLM API calls)
- **ANTHROPIC_API_KEY** or **OPENAI_API_KEY** environment variable (recommended; falls back to heuristic analysis)

## Security

- **Read-only by default.** The skill reads source files and generates a report. It never auto-applies code changes.
- **Fixes are suggestions only.** `--fix` outputs unified diffs for human review.
- **LLM usage is explicit.** If an API key is configured, file content may be sent to that provider for analysis.
- See **[SECURITY.md](SECURITY.md)** for the full security model and abuse-case mitigations.

## Testing

- Run syntax checks and smoke tests before publishing.
- Full test plan: **[TESTING.md](TESTING.md)**.

## Sin Categories

| # | Category | Weight | Description |
|:-:|----------|:------:|-------------|
| 1 | ğŸ›¡ï¸ Error Handling | 20% | Missing try/catch, bare exceptions, no edge cases |
| 2 | ğŸ“‹ Duplication | 15% | Copy-pasted logic, DRY violations |
| 3 | ğŸ’€ Dead Code | 10% | Unused imports, commented-out blocks, unreachable code |
| 4 | ğŸ” Input Validation | 15% | No type checks, no bounds checks, trusting all input |
| 5 | ğŸ”® Magic Values | 10% | Hardcoded numbers/strings/URLs without named constants |
| 6 | ğŸ§ª Test Coverage | 10% | No test files, no assertions, no test patterns |
| 7 | ğŸ“› Naming Quality | 10% | Vague names (data, result, temp, x), misleading names |
| 8 | ğŸ”’ Security | 10% | eval(), exec(), hardcoded secrets, SQL injection |

## Scoring

| Grade | Range | Meaning |
|:-----:|:-----:|---------|
| A | 90-100 | Pristine code. Ship it! ğŸš€ |
| B | 80-89 | Clean code with minor issues |
| C | 70-79 | Decent but some lazy patterns |
| D | 60-69 | Needs human attention |
| F | <60 | Heavy vibe coding detected ğŸš¨ |

## Example Output

```
# ğŸ­ Vibe Check Report

**Project:** `src/`
**Score:** 42/100 (Grade: F)
**Files analyzed:** 5
**Total lines:** 847
**Verdict:** Heavy vibe coding detected. This codebase needs serious human review. ğŸš¨

![Vibe Score](https://img.shields.io/badge/vibe--score-42%2F100-red)

## ğŸ“Š Score Breakdown

| Category | Score | Weight | Issues |
|----------|:-----:|:------:|:------:|
| ğŸ›¡ï¸ Error Handling | 25 | 20% | 8 |
| ğŸ“‹ Duplication | 50 | 15% | 4 |
| ğŸ’€ Dead Code | 65 | 10% | 3 |
| ğŸ” Input Validation | 20 | 15% | 7 |
| ğŸ”® Magic Values | 45 | 10% | 5 |
| ğŸ§ª Test Coverage | 0 | 10% | 0 test files |
| ğŸ“› Naming Quality | 55 | 10% | 3 |
| ğŸ”’ Security | 30 | 10% | 4 |

## ğŸ“ˆ Category Bars

\`\`\`
Error Handling       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  25/100
Duplication          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  50/100
Dead Code            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘  65/100
Input Validation     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  20/100
Magic Values         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  45/100
Test Coverage        â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0/100
Naming Quality       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  55/100
Security             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  30/100
\`\`\`

## ğŸ” Top Findings

### ğŸ”´ Critical
1. **api/handler.py:42** â€” `eval(user_input)` â€” Remote code execution vulnerability
2. **utils/db.py:88** â€” `f"SELECT * FROM users WHERE id={user_id}"` â€” SQL injection via f-string
3. **auth/login.py:15** â€” `password = "admin123"` â€” Hardcoded credential

### ğŸŸ¡ Warning
4. **api/handler.py:15-67** â€” Entire function has no error handling (53 lines)
5. **utils/helpers.py:23** â€” `data = process(x)` â€” Vague variable names throughout
6. **api/handler.py:90** â€” `# result = old_function(data)` â€” Commented-out code block

### ğŸ”µ Info
7. **utils/helpers.py:1** â€” `import os, sys, json, re` â€” Unused imports: os, re
8. **api/handler.py:30** â€” `timeout = 30` â€” Magic number without named constant

## ğŸ“ Per-File Breakdown

| File | Score | Grade | Lines | Top Issue |
|------|:-----:|:-----:|------:|-----------|
| `api/handler.py` | 22 | F | 203 | eval() with user input â€” RCE vuln... |
| `utils/db.py` | 35 | F | 156 | SQL string concatenation with f-stri... |
| `auth/login.py` | 45 | F | 89 | Hardcoded credentials found... |
| `utils/helpers.py` | 62 | D | 234 | Vague variable names throughout... |
| `models/user.py` | 78 | C | 165 | Missing input validation on email... |

## ğŸ”§ Suggested Fixes

### Fix #1: api/handler.py:42
> Use of eval() â€” potential code injection vulnerability

\`\`\`diff
--- a/api/handler.py
+++ b/api/handler.py
@@ -41,2 +41,3 @@
-    result = eval(user_input)
+    import ast
+    result = ast.literal_eval(user_input)
\`\`\`

### Fix #2: utils/db.py:88
> SQL injection via f-string concatenation

\`\`\`diff
--- a/utils/db.py
+++ b/utils/db.py
@@ -87,2 +87,2 @@
-    query = f"SELECT * FROM users WHERE id={user_id}"
+    query = "SELECT * FROM users WHERE id=%s"
-    cursor.execute(query)
+    cursor.execute(query, (user_id,))
\`\`\`

---

*ğŸ­ Vibe Check v0.1.1 â€” 34 findings across 5 files*
*Badge for your README:* `![Vibe Score](https://img.shields.io/badge/vibe--score-42%2F100-red)`
```

## Badge for Your README

After running a vibe check, copy the badge Markdown into your README:

```markdown
![Vibe Score](https://img.shields.io/badge/vibe--score-73%2F100-yellow)
```

Color mapping:
- ğŸŸ¢ **brightgreen** â€” A (90-100)
- ğŸŸ¢ **green** â€” B (80-89)
- ğŸŸ¡ **yellow** â€” C (70-79)
- ğŸŸ  **orange** â€” D (60-69)
- ğŸ”´ **red** â€” F (<60)

## How It Works

1. **File Discovery** â€” Finds all `.py`, `.ts`, `.tsx`, `.js`, `.jsx` files (skips `node_modules`, `dist`, `__pycache__`, etc.)
2. **LLM Analysis** â€” Sends each file to an LLM with a structured audit prompt targeting the 8 sin categories
3. **Scoring** â€” Each file gets a 0-100 score per category; overall score is a weighted average across files (weighted by line count)
4. **Report** â€” Generates a Markdown scorecard with tables, bar charts, findings, and optional fix patches

### Without an LLM API Key

The tool falls back to heuristic analysis using regex patterns and code structure checks. It's less accurate but still catches common issues like:
- Missing try/catch blocks
- `eval()` / `exec()` usage
- Magic numbers
- Vague variable names
- Commented-out code blocks
- Missing test patterns

## Supported Languages (v1)

- **Python** (`.py`)
- **TypeScript** (`.ts`, `.tsx`)
- **JavaScript** (`.js`, `.jsx`, `.mjs`, `.cjs`)

## Environment Variables

| Variable | Description |
|----------|-------------|
| `ANTHROPIC_API_KEY` | Anthropic API key for Claude (recommended) |
| `OPENAI_API_KEY` | OpenAI API key for GPT-4o |
| `VIBE_CHECK_BATCH_SIZE` | Files per LLM batch (default: 3) |

## More from CacheForge

This skill is part of the **CacheForge** open skill suite.

| Skill | What it does |
|-------|-------------|
| **[cacheforge](https://clawhub.com/cacheforge/cacheforge)** | Connect OpenClaw to CacheForge (setup, billing, stats). |
| **[vibe-check](https://clawhub.com/cacheforge/vibe-check)** | This skill â€” AI code quality + security review scorecard. |
| **[rug-checker](https://clawhub.com/cacheforge/rug-checker)** | Solana token rug-pull risk analysis. |
| **[dep-audit](https://clawhub.com/cacheforge/dep-audit)** | Dependency vulnerability auditing across npm/pip/cargo/go. |
| **[prom-query](https://clawhub.com/cacheforge/prom-query)** | Prometheus metrics + alert triage from natural language. |

Start with:

```bash
clawhub install cacheforge
```

## License

MIT

---

Built by **[CacheForge](https://app.anvil-ai.io/)**.

CacheForge is an OpenAI-compatible gateway for agent workflows that can reduce cost and improve repeat-turn performance (results vary by provider/workload).

Vault Mode (Pro) targets tool-heavy agents. Verify savings in the CacheForge dashboard.
